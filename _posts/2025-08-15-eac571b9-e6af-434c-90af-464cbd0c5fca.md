---
title: 코히어 투자·블루스카이 규정 개정·대형 플랫폼 AI 윤리 공백
date: 2025-08-15 20:00:09 +0900
categories: [뉴스, IT]
tags: []
---

## 코히어 5억 달러 유치와 기업용 LLM의 전략적 의미

토론토에 본사를 둔 코히어가 5억 달러 규모의 자금을 모집해 기업가치가 68억 달러로 평가됐다. 이전 라운드와 동일한 금액을 모은 가운데 가치가 상승한 점이 눈에 띈다. 설립자는 Aidan Gomez로, 현대 인공지능의 핵심인 트랜스포머 구조의 원저자 가운데 한 명이다. 코히어는 초기부터 범용 소비자 서비스가 아닌 기업 고객을 겨냥한 대형언어모델을 개발해 왔다.

기업 고객을 공략하는 전략은 두 가지 축에서 정교하게 구성된다. 첫째, 데이터 보안과 규정 준수를 전면에 내세운다. 금융, 공공, 의료 등 민감한 데이터를 다루는 조직은 모델 운영에서 데이터의 위치, 접근 통제, 감사 추적을 매우 중요하게 본다. 코히어는 이런 요구에 맞춘 보안 기능과 온프레미스 또는 지정된 클라우드에의 배포 옵션을 제공하며 이를 경쟁우위로 삼고 있다. 둘째, 엔터프라이즈 소프트웨어 생태계와의 제휴로 채널을 확보한다. Oracle, Dell, Fujitsu, SAP, LG CNS 등 대형 기업 서비스 사업자들과의 협력이 있다고 알려져 있다. 이러한 파트너십은 판매 경로 확보뿐 아니라 엔터프라이즈 고객의 신뢰를 확보하는 수단으로 작용한다.

이번 라운드에는 Radical Ventures와 Inovia Capital이 주도적으로 참여했고 AMD 벤처스, 엔비디아, Salesforce 벤처스 등 기존 투자자도 참여했다. 인재 영입도 눈에 띈다. 메타의 연구 책임자였던 Joelle Pineau를 최고 AI 책임자로 데려오고 재무 책임자로 Francois Chadwick를 영입했다. 이 같은 채용은 기술력과 재무 운영 능력을 동시에 강화하려는 의도로 읽힌다.

현재 대형언어모델 시장은 소수의 대형 플레이어가 주도하고 있다. OpenAI, Anthropic, Meta 등은 소비자와 엔터프라이즈 사이에서 폭넓은 제품 라인을 전개하고 있다. 그 속에서 코히어의 포지셔닝은 '기업 특화, 보안 중심'이라는 명확한 차별화다. 기업들은 자체 데이터로 모델을 학습시키거나 맞춤형 파인튜닝을 원한다. 이 과정에서 데이터 프라이버시와 규정 준수 요구사항이 커지며, 그 요구를 충족하는 솔루션에 대한 수요는 계속 성장할 가능성이 크다.

의미 있는 시사점이 몇 가지 있다. 첫째, 자금 조달은 단순한 현금 확보를 넘어 산업의 인정 표시로 기능한다. 기술적 신뢰와 시장 확장에 대한 기대가 밸류에이션에 반영된다. 둘째, 엔터프라이즈 시장에서의 경쟁은 기술 성능뿐 아니라 보안, 거버넌스, 파트너 에코시스템이 핵심 변수임을 재확인시킨다. 셋째, 대형 인재 채용과 파트너십은 단기간의 제품 개선을 넘어 장기적 고객 신뢰 구축에 기여한다.

리스크 요인도 존재한다. 모델 성능 경쟁은 치열하고 대형 클라우드 사업자 및 모델 제공자가 통합 솔루션을 제공할 여지도 크다. 규제 환경은 지역별로 달라 기업 고객이 요구하는 조건이 복잡하다. 또한 고성능 하드웨어와 데이터 처리 인프라에 대한 의존도가 크다. 기술 표준과 상호운용성이 중요한 변수가 될 전망이다.

기업용 모델의 확산은 산업 전반의 업무 방식과 서비스 제공 형태에 영향을 준다. 고객 상담, 문서 분석, 규정 준수 자동화 등에서 효율을 얻는 한편 윤리적 운영과 투명성 확보가 함께 요구된다. 코히어의 사례는 기술 기업이 시장에서 차별화하려면 단순한 모델 성능을 넘어 신뢰와 규범, 산업 특화 기능을 통합해야 한다는 점을 보여준다.

## 블루스카이의 커뮤니티 가이드라인 개정과 규제 대응의 균형

출시 후 2년이 지난 블루스카이가 커뮤니티 가이드라인과 관련 정책을 개정하고 사용자 의견을 수렴한다. 경쟁 관계에 있는 다른 소셜 서비스들과 달리 블루스카이는 개정안에서 규제 준수를 명확히 하고 사용자 안전과 기타 권리 보호를 강화하려는 의지를 드러낸다. 이번 개정은 영국의 온라인 안전법, 유럽 연합의 디지털 서비스법, 미국의 관련 입법 제안 등 새로운 법적 요구를 반영하려는 조치가 큰 동력으로 작용했다.

중요한 변경 중 하나는 연령 확인에 관한 규정이다. 영국의 규제는 성인용 콘텐츠를 제공하는 플랫폼에 연령 검증을 요구한다. 이에 따라 사용자에게 얼굴인식, 신분증 업로드 또는 결제 카드 인증 등의 절차를 제시할 수 있다. 연령 확인은 미성년자 보호라는 공익적 목적을 가진다. 그러나 개인정보 수집과 보안, 신원정보의 저장 및 처리 방식에 대한 우려를 동반한다. 사용자 프라이버시와 규제 준수 요구 사이의 균형이 핵심 문제로 부상한다.

분쟁 처리 절차도 구체화됐다. 플랫폼 내부의 불만 제기와 이의 제기에 대해 보다 상세한 흐름을 제시하고, 비공식적 해결 절차를 통해 전화상담 등으로 사안을 먼저 다루겠다는 접근을 채택한다. 이는 대형 소셜 서비스에서 흔히 발생하는 이용자 불만 처리의 불투명성을 완화하려는 시도로 읽힌다. 사용자가 정식 제소 전에 플랫폼과 직접 대화할 기회를 갖게 되면 오해가 해소되고 과도한 제재가 줄어들 가능성이 있다.

가이드라인은 네 가지 원칙을 중심으로 구성된다. 안전을 우선시하고 타인을 존중하며 진정성을 유지하고 규칙을 준수한다. 폭력 조장, 불법 행위, 비동의 개인정보 유포, 스팸 등 일반적으로 금지되는 행위가 명시돼 있다. 언론 보도나 패러디에 대한 예외 규정도 마련돼 표현의 자유와 공익성 있는 보도가 보호되도록 설계됐다.

법원으로의 접근을 허용하는 점은 주목할 만하다. 플랫폼이 분쟁 해결을 민사 소송 대신 중재로 유도하는 관행을 자주 보완해 왔다. 블루스카이는 일부 피해 주장에 대해 이용자가 법원에 직접 제소할 수 있도록 허용함으로써 이용자의 권리 침해에 대한 구제 가능성을 넓혔다. 이는 분쟁 해결의 투명성과 책임 소재 규명에 기여할 수 있다.

여러 가지 난제는 여전히 남아 있다. 첫째, 연령 확인 절차는 프라이버시와 접근성 문제를 일으킬 수 있다. 일부 이용자는 신원 확인 방식이 과도하다고 느끼고 가입 장벽으로 작용할 수 있다. 둘째, 플랫폼이 커뮤니티 성향을 '더 친절하고 존중하는 방향'으로 유도하려는 의도는 규범 형성과 표현의 자유 사이의 민감한 경계에 놓인다. 내부 규정의 적용 과정에서 일관성과 투명성이 확보되지 않으면 특정 집단의 불만을 낳을 수 있다. 셋째, 글로벌 법령의 차이를 반영한 정책 설계는 운영상의 복잡성을 키운다. 지역별로 다른 규정을 어떻게 기술적으로 구현하고 운영정책으로 일관되게 적용할지에 대한 기술적, 인적 자원이 필요하다.

사회적 의미는 분명하다. 소셜 네트워크의 규칙은 단순한 서비스 이용 규범을 넘어 공적 담론의 형성과 사회적 상호작용의 방식에 영향을 미친다. 작은 설계 선택 하나가 커뮤니티의 분위기와 참여 형태를 바꿀 수 있다. 플랫폼이 자율적 도구를 제공해 사용자 개개인이 커뮤니티를 만들도록 유도하는 방향과 플랫폼이 보다 적극적으로 규범을 설정하는 방향 사이의 균형을 찾는 일이 앞으로도 중요한 과제로 남는다.

## 대형 플랫폼의 내부 AI 기준과 실제 운영 간 괴리

대형 플랫폼의 내부 문서가 외부로 알려지며 운영 원칙과 실제 적용 사이의 간극이 드러났다. 내부 기준에는 각종 사례별 허용과 금지 규정이 샘플 문장 형태로 제시돼 있다. 문제는 문서에 포함된 예시들이 공공의 기대와 동떨어진 해석을 낳을 수 있다는 점이다. 문서 작성과 승인 과정에는 법무, 정책, 기술 부서와 윤리 책임자가 참여한 것으로 알려졌다. 이후 일부 문구가 부적절하다는 지적을 받아 수정 또는 삭제됐다고 회사는 밝혔다.

문제의 핵심은 세 가지다. 첫째, AI 대화형 시스템이 만들어내는 표현의 범위를 어디까지 허용할지에 대한 내부 판단 기준의 불명확성이다. 둘째, 사용자와 감정적으로 상호작용하도록 설계된 챗봇이 사람에게 미치는 심리적 영향을 과소평가할 우려가 있다. 셋째, 생성되는 정보의 사실성 관리가 부족할 때 오정보가 확산될 수 있다.

정책 문서에는 정보가 사실이 아님을 명시하는 경우에 한해 허용될 수 있다는 규정이 있지만 실무에서 일관되게 적용되는지는 별개의 문제다. 챗봇이 허구의 서사를 만들어내거나 사용자의 감정에 호소하는 방식으로 동작할 경우 사용자의 판단을 흐리게 만들 위험이 있다. 특히 감정적 취약성이 있는 이용자들이 AI와의 상호작용에서 현실과 허구를 구분하기 어려워질 때 발생하는 사회적 비용이 크다.

안전성 확보를 위해서는 여러 보완 장치가 필요하다. 명확한 허용 기준과 금지 기준을 만들고 공개하는 일은 신뢰 회복의 출발점이다. 외부 전문가에 의한 독립적 검토와 정기적 안전성 평가, 투명한 사고 보고 및 수정 조치의 공표가 필요하다. 기술적 측면에서는 모델의 응답을 생성하기 전후로 필터링과 사실 확인 절차를 넣고 자동화된 위험 탐지 시스템을 강화해야 한다. 사용자와의 상호작용 기록에 대한 감사 가능성 확보와 인간 심사자의 개입 지점을 명확히 하는 것도 중요하다.

사회적 반응은 이미 나타나고 있다. 아동 안전 단체와 이용자 보호 단체들은 명확한 가이드라인 공개와 외부 검증을 요구한다. 연구자들은 AI 동반자 서비스의 심리적 영향에 대한 장기 연구를 촉구한다. 규제 기관은 투명성과 책임성을 기준으로 한 감독을 강화할 가능성이 크다.

대형 플랫폼이 인공지능을 매개로 제공하는 새로운 형태의 서비스는 기술적 편익과 함께 윤리적, 사회적 책임을 동시에 요구한다. 내부 문서 하나의 표현이 즉각적으로 공중의 신뢰로 연결되지 않는다는 점을 기업들이 체감해야 한다. 정책과 실제 운영의 간극을 줄이는 일이 기술 발전의 지속 가능성을 좌우하는 핵심 변수가 될 것이다.